{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import rasa_nlu,rasa_nlu,spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.evaluate import run_evaluation\n",
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_core.events import SlotSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'abb_expand_md' (str) to file 'abbexp.md'.\n"
     ]
    }
   ],
   "source": [
    "abb_expand_md=\"\"\"\n",
    "\n",
    "## intent:greet\n",
    "- hey\n",
    "- hello there\n",
    "- hi\n",
    "- hello there\n",
    "- good morning\n",
    "- good evening\n",
    "- hey there\n",
    "- let's go\n",
    "- hey dude\n",
    "- goodmorning\n",
    "- goodevening\n",
    "- good afternoon\n",
    "\n",
    "## intent:goodbye\n",
    "- cu\n",
    "- good by\n",
    "- cee you later\n",
    "- good night\n",
    "- good afternoon\n",
    "- bye\n",
    "- goodbye\n",
    "- have a nice day\n",
    "- see you around\n",
    "- bye bye\n",
    "- see you later\n",
    "\n",
    "## intent: abbresearch\n",
    "- I would like to know the abbrevation\n",
    "- What is the abbrevation for\n",
    "- abbrevation\n",
    "\n",
    "## intent: expansion\n",
    "- I want to know the expansion for\n",
    "- What is the expansion for\n",
    "- expansion\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "%store abb_expand_md > abbexp.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'config_c' (str) to file 'config_c.yml'.\n"
     ]
    }
   ],
   "source": [
    "config_c =\"\"\"\n",
    "language: \"en\"\n",
    "\n",
    "pipeline:\n",
    "- name: \"nlp_spacy\"                   # loads the spacy language model\n",
    "- name: \"tokenizer_spacy\"             # splits the sentence into tokens\n",
    "- name: \"ner_crf\"                   # uses the pretrained spacy NER model\n",
    "- name: \"intent_featurizer_spacy\"     # transform the sentence into a vector representation\n",
    "- name: \"intent_classifier_sklearn\"   # uses the vector representation to classify using SVM\n",
    "- name: \"ner_synonyms\"                # trains the synonyms\n",
    "\"\"\" \n",
    "\n",
    "%store config_c > config_c.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= load_data(\"abbexp.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from rasa_nlu import config\n",
    "\n",
    "# trainer to educate our pipeline\n",
    "trainer = Trainer(config.load(\"config_c.yml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "interpreter = trainer.train(train_data)\n",
    "model_directory = trainer.persist(\"./models/EA\", fixed_model_name=\"current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"intent\": {\n",
      "    \"name\": \"abbresearch\",\n",
      "    \"confidence\": 0.6470515815285962\n",
      "  },\n",
      "  \"entities\": [],\n",
      "  \"intent_ranking\": [\n",
      "    {\n",
      "      \"name\": \"abbresearch\",\n",
      "      \"confidence\": 0.6470515815285962\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"greet\",\n",
      "      \"confidence\": 0.22180733754907658\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"goodbye\",\n",
      "      \"confidence\": 0.09917254546469297\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"expansion\",\n",
      "      \"confidence\": 0.03196853545763417\n",
      "    }\n",
      "  ],\n",
      "  \"text\": \"abbrevation\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def output(o):\n",
    "    print(json.dumps(o,indent=2))\n",
    "    \n",
    "output(interpreter.parse(\"abbrevation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetchall:\n",
      "('1NAM ', ' One in a million')\n",
      "('143', 'I love you')\n",
      "('1432', ' I love you too')\n",
      "('2moro ', 'Tomorrow')\n",
      "('2N8', 'Tonight')\n",
      "('404', 'No clue')\n",
      "('4Eva', 'Forever')\n",
      "('ASAP', 'As soon as possible')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"expand1.db\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM ea\")\n",
    "print(\"fetchall:\")\n",
    "result = cursor.fetchall()\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\rasa_nlu\\extractors\\entity_synonyms.py:85: UserWarning: Failed to load synonyms file from 'D:\\Jupyter\\Text Generation - RNN\\Abbrevatin_exp_bot\\./models/EA\\default\\current\\entity_synonyms.json'\n",
      "  \"\".format(entity_synonyms_file))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_evaluation(\"abbexp.md\",model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'story_md' (str) to file 'story.md'.\n"
     ]
    }
   ],
   "source": [
    "story_md =\"\"\"\n",
    "\n",
    " \n",
    "## concept path          \n",
    "* greet\n",
    "  - utter_greet\n",
    "* ask_expansion\n",
    "    -utter_ask_expansion\n",
    "* inform{\"group\":\"expansion\"}\n",
    "    - action_retrieve_expansion\n",
    "* ask_abbrevation\n",
    "  - utter_ask_abrevation\n",
    "* inform{\"group\":\"abbrevation\"}  \n",
    "  - action_retrieve_expansion\n",
    "* mood_affirm\n",
    "  - utter_happy\n",
    "  \n",
    "## fallback\n",
    "- utter_unclear   \n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "  \n",
    "%store story_md >story.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'domain_yml' (str) to file 'domain.yml'.\n"
     ]
    }
   ],
   "source": [
    "domain_yml = \"\"\"\n",
    "intents:\n",
    "- greet\n",
    "- goodbye\n",
    "- mood_affirm\n",
    "- mood_great\n",
    "- inform\n",
    "\n",
    "slots:\n",
    "  group:\n",
    "    type: text\n",
    "    \n",
    "entities:\n",
    "- group\n",
    "\n",
    "actions:\n",
    "- utter_greet\n",
    "- utter_happy\n",
    "- utter_unclear\n",
    "- utter_ask_abrevation\n",
    "- utter_ask_expansion\n",
    "- __main__.ApiAction\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "  - text: \"Hey! How are you?\"\n",
    "\n",
    "  utter_unclear:\n",
    "  - text: \"I am not sure what you are aiming for.\"\n",
    "  \n",
    "  utter_happy:\n",
    "  - text: \"Great carry on!\"\n",
    "\n",
    "  utter_ask_abrevation:\n",
    "  - text: \"Would u like to know the abbrevation for something?\"\n",
    "  \n",
    "  utter_ask_expansion:\n",
    "  - text: \"Would u like to know the expansion for something?\"\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "%store domain_yml > domain.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_core.actions import Action\n",
    "from rasa_core.events import SlotSet\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "class ApiAction(Action):\n",
    "    def name(self):\n",
    "        return \"action_retrieve_expansion\"\n",
    "   ## def name(self):\n",
    "       ## return \"action_retrieve_abbrevation\"\n",
    "\n",
    "    def run(self, dispatcher, tracker, domain):\n",
    "        \n",
    "        group = tracker.get_slot('group')\n",
    "          \n",
    "        q = \"SELECT expansion FROM ea WHERE group='{0}'limit 1\".format(group)\n",
    "        result = cursor.execute(q)\n",
    "        if result:\n",
    "            return print(cursor.fetchone())\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##return [SlotSet(\"matches\", result if result is not None else cursor.fetchone())]\n",
    "        \n",
    "        #return [SlotSet(\"matches\", result if result is not None else [])]\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiAction(Action):\n",
    "    def name(self):\n",
    "        return \"action_retrieve_expansion\"\n",
    "   ## def name(self):\n",
    "       ## return \"action_retrieve_abbrevation\"\n",
    "\n",
    "    def run(self, dispatcher, tracker, domain):\n",
    "        \n",
    "        group = tracker.get_slot('group')\n",
    "          \n",
    "        q = \"SELECT expansion FROM ea WHERE group='{0}'limit 1\".format(group)\n",
    "        result = cursor.execute(q)\n",
    "        if result:\n",
    "            return print(cursor.fetchone())\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 673.57it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 501.29it/s, # trackers=2]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 505.79it/s, # trackers=4]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 334.15it/s, # trackers=6]\n",
      "Processed trackers: 100%|████████████████████████████████████████████████| 20/20 [00:00<00:00, 51.29it/s, # actions=44]\n",
      "Processed actions: 44it [00:00, 1026.02it/s, # examples=44]\n",
      "Processed trackers: 100%|████████████████████████████████████████████████| 20/20 [00:00<00:00, 40.35it/s, # actions=44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 5, 15)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6144      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 6,408\n",
      "Trainable params: 6,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.1518 - acc: 0.218 - 1s 13ms/step - loss: 2.1333 - acc: 0.1818\n",
      "Epoch 2/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0696 - acc: 0.187 - 0s 295us/step - loss: 2.0635 - acc: 0.1818\n",
      "Epoch 3/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0313 - acc: 0.218 - 0s 295us/step - loss: 2.0422 - acc: 0.2500\n",
      "Epoch 4/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0236 - acc: 0.187 - 0s 272us/step - loss: 2.0118 - acc: 0.2727\n",
      "Epoch 5/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.9831 - acc: 0.375 - 0s 317us/step - loss: 1.9782 - acc: 0.4318\n",
      "Epoch 6/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.9280 - acc: 0.562 - 0s 272us/step - loss: 1.9424 - acc: 0.6136\n",
      "Epoch 7/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.9197 - acc: 0.500 - 0s 317us/step - loss: 1.9322 - acc: 0.5455\n",
      "Epoch 8/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8934 - acc: 0.562 - 0s 317us/step - loss: 1.8782 - acc: 0.6136\n",
      "Epoch 9/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8762 - acc: 0.531 - 0s 272us/step - loss: 1.8564 - acc: 0.6136\n",
      "Epoch 10/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8030 - acc: 0.625 - 0s 227us/step - loss: 1.8199 - acc: 0.5682\n",
      "Epoch 11/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8454 - acc: 0.500 - 0s 227us/step - loss: 1.8319 - acc: 0.5682\n",
      "Epoch 12/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8158 - acc: 0.531 - 0s 249us/step - loss: 1.7813 - acc: 0.5909\n",
      "Epoch 13/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7615 - acc: 0.593 - 0s 295us/step - loss: 1.7688 - acc: 0.5682\n",
      "Epoch 14/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7712 - acc: 0.531 - 0s 340us/step - loss: 1.7567 - acc: 0.5682\n",
      "Epoch 15/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6619 - acc: 0.593 - 0s 317us/step - loss: 1.7247 - acc: 0.5682\n",
      "Epoch 16/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6984 - acc: 0.593 - 0s 295us/step - loss: 1.7110 - acc: 0.5682\n",
      "Epoch 17/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6399 - acc: 0.625 - 0s 295us/step - loss: 1.6841 - acc: 0.5682\n",
      "Epoch 18/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7390 - acc: 0.468 - 0s 272us/step - loss: 1.6358 - acc: 0.5682\n",
      "Epoch 19/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6586 - acc: 0.531 - 0s 295us/step - loss: 1.6187 - acc: 0.5682\n",
      "Epoch 20/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6036 - acc: 0.593 - 0s 295us/step - loss: 1.6184 - acc: 0.5682\n",
      "Epoch 21/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5754 - acc: 0.500 - 0s 317us/step - loss: 1.5628 - acc: 0.5682\n",
      "Epoch 22/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5099 - acc: 0.625 - 0s 295us/step - loss: 1.5491 - acc: 0.5682\n",
      "Epoch 23/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5824 - acc: 0.562 - 0s 249us/step - loss: 1.5451 - acc: 0.5682\n",
      "Epoch 24/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4846 - acc: 0.687 - 0s 295us/step - loss: 1.5347 - acc: 0.5682\n",
      "Epoch 25/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5286 - acc: 0.562 - 0s 272us/step - loss: 1.5568 - acc: 0.5682\n",
      "Epoch 26/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6335 - acc: 0.500 - 0s 249us/step - loss: 1.4740 - acc: 0.5682\n",
      "Epoch 27/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5148 - acc: 0.593 - 0s 295us/step - loss: 1.4919 - acc: 0.5682\n",
      "Epoch 28/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5587 - acc: 0.531 - 0s 249us/step - loss: 1.4634 - acc: 0.5682\n",
      "Epoch 29/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4270 - acc: 0.531 - 0s 295us/step - loss: 1.4092 - acc: 0.5682\n",
      "Epoch 30/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4691 - acc: 0.562 - 0s 295us/step - loss: 1.4801 - acc: 0.5682\n",
      "Epoch 31/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3884 - acc: 0.593 - 0s 295us/step - loss: 1.4114 - acc: 0.5682\n",
      "Epoch 32/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4345 - acc: 0.531 - 0s 272us/step - loss: 1.3851 - acc: 0.5682\n",
      "Epoch 33/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3805 - acc: 0.593 - 0s 227us/step - loss: 1.4014 - acc: 0.5682\n",
      "Epoch 34/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5114 - acc: 0.468 - 0s 272us/step - loss: 1.3781 - acc: 0.5682\n",
      "Epoch 35/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3578 - acc: 0.625 - 0s 249us/step - loss: 1.3856 - acc: 0.5682\n",
      "Epoch 36/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2496 - acc: 0.656 - 0s 272us/step - loss: 1.4061 - acc: 0.5682\n",
      "Epoch 37/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4084 - acc: 0.562 - 0s 272us/step - loss: 1.3703 - acc: 0.5682\n",
      "Epoch 38/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3805 - acc: 0.593 - 0s 249us/step - loss: 1.4043 - acc: 0.5682\n",
      "Epoch 39/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4544 - acc: 0.531 - 0s 249us/step - loss: 1.3877 - acc: 0.5682\n",
      "Epoch 40/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4607 - acc: 0.531 - 0s 249us/step - loss: 1.3581 - acc: 0.5682\n",
      "Epoch 41/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3251 - acc: 0.531 - 0s 249us/step - loss: 1.3168 - acc: 0.5682\n",
      "Epoch 42/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3548 - acc: 0.593 - 0s 227us/step - loss: 1.3582 - acc: 0.5682\n",
      "Epoch 43/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1784 - acc: 0.625 - 0s 295us/step - loss: 1.3041 - acc: 0.5682\n",
      "Epoch 44/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2430 - acc: 0.625 - 0s 295us/step - loss: 1.3132 - acc: 0.5682\n",
      "Epoch 45/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1992 - acc: 0.593 - 0s 227us/step - loss: 1.2792 - acc: 0.5682\n",
      "Epoch 46/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3237 - acc: 0.500 - 0s 249us/step - loss: 1.2637 - acc: 0.5682\n",
      "Epoch 47/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3416 - acc: 0.500 - 0s 249us/step - loss: 1.2602 - acc: 0.5682\n",
      "Epoch 48/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2705 - acc: 0.625 - 0s 272us/step - loss: 1.2998 - acc: 0.5682\n",
      "Epoch 49/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2744 - acc: 0.593 - 0s 249us/step - loss: 1.2902 - acc: 0.5682\n",
      "Epoch 50/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0887 - acc: 0.625 - 0s 227us/step - loss: 1.2412 - acc: 0.5682\n",
      "Epoch 51/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1097 - acc: 0.656 - 0s 272us/step - loss: 1.2526 - acc: 0.5682\n",
      "Epoch 52/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3208 - acc: 0.562 - 0s 340us/step - loss: 1.2340 - acc: 0.5682\n",
      "Epoch 53/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2451 - acc: 0.531 - 0s 295us/step - loss: 1.2167 - acc: 0.5682\n",
      "Epoch 54/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3056 - acc: 0.531 - 0s 295us/step - loss: 1.2417 - acc: 0.5682\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 1.2785 - acc: 0.562 - 0s 317us/step - loss: 1.2496 - acc: 0.5682\n",
      "Epoch 56/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1726 - acc: 0.625 - 0s 340us/step - loss: 1.2261 - acc: 0.5682\n",
      "Epoch 57/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2347 - acc: 0.562 - 0s 249us/step - loss: 1.2143 - acc: 0.5682\n",
      "Epoch 58/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2301 - acc: 0.562 - 0s 249us/step - loss: 1.2247 - acc: 0.5682\n",
      "Epoch 59/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1948 - acc: 0.593 - 0s 272us/step - loss: 1.1751 - acc: 0.5682\n",
      "Epoch 60/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2507 - acc: 0.531 - 0s 272us/step - loss: 1.1974 - acc: 0.5909\n",
      "Epoch 61/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0972 - acc: 0.593 - 0s 295us/step - loss: 1.1531 - acc: 0.5682\n",
      "Epoch 62/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2456 - acc: 0.531 - 0s 272us/step - loss: 1.1588 - acc: 0.5682\n",
      "Epoch 63/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1286 - acc: 0.593 - 0s 249us/step - loss: 1.1303 - acc: 0.5682\n",
      "Epoch 64/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1418 - acc: 0.593 - 0s 272us/step - loss: 1.1394 - acc: 0.5909\n",
      "Epoch 65/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0825 - acc: 0.593 - 0s 272us/step - loss: 1.1127 - acc: 0.5909\n",
      "Epoch 66/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0429 - acc: 0.656 - 0s 317us/step - loss: 1.1062 - acc: 0.5909\n",
      "Epoch 67/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0459 - acc: 0.687 - 0s 272us/step - loss: 1.1457 - acc: 0.5682\n",
      "Epoch 68/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1457 - acc: 0.593 - 0s 272us/step - loss: 1.1371 - acc: 0.5682\n",
      "Epoch 69/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0956 - acc: 0.593 - 0s 272us/step - loss: 1.0958 - acc: 0.5909\n",
      "Epoch 70/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1536 - acc: 0.562 - 0s 295us/step - loss: 1.1061 - acc: 0.5682\n",
      "Epoch 71/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1495 - acc: 0.531 - 0s 272us/step - loss: 1.0861 - acc: 0.5682\n",
      "Epoch 72/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0804 - acc: 0.531 - 0s 272us/step - loss: 1.0413 - acc: 0.5909\n",
      "Epoch 73/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0462 - acc: 0.562 - 0s 227us/step - loss: 1.0541 - acc: 0.5682\n",
      "Epoch 74/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0427 - acc: 0.625 - 0s 249us/step - loss: 1.0805 - acc: 0.5909\n",
      "Epoch 75/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0485 - acc: 0.593 - 0s 249us/step - loss: 1.0590 - acc: 0.5909\n",
      "Epoch 76/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0004 - acc: 0.593 - 0s 249us/step - loss: 1.0138 - acc: 0.5909\n",
      "Epoch 77/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0117 - acc: 0.593 - 0s 272us/step - loss: 1.0551 - acc: 0.5909\n",
      "Epoch 78/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9149 - acc: 0.625 - 0s 249us/step - loss: 1.0162 - acc: 0.5909\n",
      "Epoch 79/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0066 - acc: 0.593 - 0s 249us/step - loss: 1.0321 - acc: 0.5909\n",
      "Epoch 80/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9891 - acc: 0.562 - 0s 227us/step - loss: 0.9914 - acc: 0.5909\n",
      "Epoch 81/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9813 - acc: 0.593 - 0s 249us/step - loss: 0.9764 - acc: 0.5909\n",
      "Epoch 82/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9364 - acc: 0.625 - 0s 272us/step - loss: 1.0249 - acc: 0.5682\n",
      "Epoch 83/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9267 - acc: 0.593 - 0s 249us/step - loss: 0.9734 - acc: 0.6136\n",
      "Epoch 84/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9217 - acc: 0.625 - 0s 272us/step - loss: 0.9957 - acc: 0.5909\n",
      "Epoch 85/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9501 - acc: 0.562 - 0s 340us/step - loss: 0.9446 - acc: 0.5909\n",
      "Epoch 86/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0084 - acc: 0.562 - 0s 317us/step - loss: 0.9635 - acc: 0.5909\n",
      "Epoch 87/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0068 - acc: 0.562 - 0s 317us/step - loss: 0.9462 - acc: 0.6136\n",
      "Epoch 88/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9663 - acc: 0.656 - 0s 317us/step - loss: 0.9509 - acc: 0.6364\n",
      "Epoch 89/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9535 - acc: 0.656 - 0s 295us/step - loss: 0.9221 - acc: 0.6591\n",
      "Epoch 90/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8634 - acc: 0.625 - 0s 363us/step - loss: 0.8954 - acc: 0.6818\n",
      "Epoch 91/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8996 - acc: 0.656 - 0s 272us/step - loss: 0.9259 - acc: 0.6591\n",
      "Epoch 92/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9301 - acc: 0.593 - 0s 295us/step - loss: 0.8800 - acc: 0.6136\n",
      "Epoch 93/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8587 - acc: 0.656 - 0s 317us/step - loss: 0.8731 - acc: 0.6591\n",
      "Epoch 94/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9273 - acc: 0.656 - 0s 295us/step - loss: 0.9158 - acc: 0.6364\n",
      "Epoch 95/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8856 - acc: 0.687 - 0s 317us/step - loss: 0.8613 - acc: 0.7045\n",
      "Epoch 96/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8125 - acc: 0.718 - 0s 272us/step - loss: 0.8717 - acc: 0.7273\n",
      "Epoch 97/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9285 - acc: 0.718 - 0s 272us/step - loss: 0.8690 - acc: 0.7045\n",
      "Epoch 98/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8284 - acc: 0.750 - 0s 272us/step - loss: 0.8595 - acc: 0.7500\n",
      "Epoch 99/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8583 - acc: 0.750 - 0s 272us/step - loss: 0.8429 - acc: 0.7273\n",
      "Epoch 100/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.9016 - acc: 0.687 - 0s 295us/step - loss: 0.8638 - acc: 0.6818\n",
      "Epoch 101/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8463 - acc: 0.718 - 0s 272us/step - loss: 0.8458 - acc: 0.7273\n",
      "Epoch 102/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8052 - acc: 0.812 - 0s 249us/step - loss: 0.8136 - acc: 0.7500\n",
      "Epoch 103/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7439 - acc: 0.781 - 0s 227us/step - loss: 0.8183 - acc: 0.7727\n",
      "Epoch 104/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7944 - acc: 0.812 - 0s 272us/step - loss: 0.7697 - acc: 0.7955\n",
      "Epoch 105/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7456 - acc: 0.781 - 0s 317us/step - loss: 0.7713 - acc: 0.7955\n",
      "Epoch 106/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8242 - acc: 0.781 - 0s 272us/step - loss: 0.7795 - acc: 0.8182\n",
      "Epoch 107/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8728 - acc: 0.687 - 0s 363us/step - loss: 0.8343 - acc: 0.7045\n",
      "Epoch 108/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8365 - acc: 0.781 - 0s 295us/step - loss: 0.7526 - acc: 0.8182\n",
      "Epoch 109/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7763 - acc: 0.718 - 0s 272us/step - loss: 0.7376 - acc: 0.7273\n",
      "Epoch 110/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8502 - acc: 0.687 - 0s 295us/step - loss: 0.7704 - acc: 0.7727\n",
      "Epoch 111/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7961 - acc: 0.718 - 0s 272us/step - loss: 0.7399 - acc: 0.7500\n",
      "Epoch 112/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6351 - acc: 0.875 - 0s 249us/step - loss: 0.7087 - acc: 0.8182\n",
      "Epoch 113/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6927 - acc: 0.812 - 0s 272us/step - loss: 0.7114 - acc: 0.8409\n",
      "Epoch 114/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7260 - acc: 0.781 - 0s 340us/step - loss: 0.7200 - acc: 0.7727\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.7884 - acc: 0.750 - 0s 227us/step - loss: 0.7391 - acc: 0.7955\n",
      "Epoch 116/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7164 - acc: 0.781 - 0s 249us/step - loss: 0.7071 - acc: 0.7500\n",
      "Epoch 117/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6526 - acc: 0.781 - 0s 272us/step - loss: 0.6850 - acc: 0.8182\n",
      "Epoch 118/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5826 - acc: 0.843 - 0s 317us/step - loss: 0.6503 - acc: 0.8182\n",
      "Epoch 119/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6446 - acc: 0.843 - 0s 272us/step - loss: 0.6336 - acc: 0.8409\n",
      "Epoch 120/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6568 - acc: 0.875 - 0s 295us/step - loss: 0.6489 - acc: 0.8409\n",
      "Epoch 121/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6077 - acc: 0.781 - 0s 295us/step - loss: 0.6453 - acc: 0.7955\n",
      "Epoch 122/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6259 - acc: 0.812 - 0s 272us/step - loss: 0.6325 - acc: 0.8409\n",
      "Epoch 123/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6030 - acc: 0.812 - 0s 295us/step - loss: 0.6236 - acc: 0.8182\n",
      "Epoch 124/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6149 - acc: 0.875 - 0s 317us/step - loss: 0.6141 - acc: 0.8864\n",
      "Epoch 125/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5301 - acc: 0.843 - 0s 317us/step - loss: 0.6019 - acc: 0.8182\n",
      "Epoch 126/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5023 - acc: 0.937 - 0s 317us/step - loss: 0.6245 - acc: 0.8864\n",
      "Epoch 127/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6705 - acc: 0.812 - 0s 385us/step - loss: 0.6473 - acc: 0.8409\n",
      "Epoch 128/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6513 - acc: 0.781 - 0s 295us/step - loss: 0.5829 - acc: 0.8182\n",
      "Epoch 129/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5498 - acc: 0.843 - 0s 317us/step - loss: 0.5654 - acc: 0.8864\n",
      "Epoch 130/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5430 - acc: 0.875 - 0s 340us/step - loss: 0.5707 - acc: 0.8636\n",
      "Epoch 131/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4940 - acc: 0.906 - 0s 295us/step - loss: 0.5855 - acc: 0.8864\n",
      "Epoch 132/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6400 - acc: 0.812 - 0s 227us/step - loss: 0.6059 - acc: 0.8409\n",
      "Epoch 133/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5673 - acc: 0.843 - 0s 249us/step - loss: 0.5301 - acc: 0.8864\n",
      "Epoch 134/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.906 - 0s 249us/step - loss: 0.5361 - acc: 0.8864\n",
      "Epoch 135/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5130 - acc: 0.906 - 0s 272us/step - loss: 0.4974 - acc: 0.9091\n",
      "Epoch 136/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4343 - acc: 0.906 - 0s 227us/step - loss: 0.4915 - acc: 0.9091\n",
      "Epoch 137/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5010 - acc: 0.906 - 0s 295us/step - loss: 0.5023 - acc: 0.9091\n",
      "Epoch 138/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4934 - acc: 0.937 - 0s 272us/step - loss: 0.5324 - acc: 0.9091\n",
      "Epoch 139/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5539 - acc: 0.875 - 0s 249us/step - loss: 0.5336 - acc: 0.8636\n",
      "Epoch 140/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6195 - acc: 0.875 - 0s 249us/step - loss: 0.5391 - acc: 0.9091\n",
      "Epoch 141/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4111 - acc: 0.906 - 0s 272us/step - loss: 0.4762 - acc: 0.8636\n",
      "Epoch 142/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5095 - acc: 0.875 - 0s 272us/step - loss: 0.4998 - acc: 0.8864\n",
      "Epoch 143/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4674 - acc: 0.906 - 0s 295us/step - loss: 0.4837 - acc: 0.9318\n",
      "Epoch 144/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4278 - acc: 0.875 - 0s 340us/step - loss: 0.4607 - acc: 0.8864\n",
      "Epoch 145/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4746 - acc: 0.875 - 0s 295us/step - loss: 0.4543 - acc: 0.9091\n",
      "Epoch 146/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5937 - acc: 0.843 - 0s 317us/step - loss: 0.4804 - acc: 0.8864\n",
      "Epoch 147/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5425 - acc: 0.937 - 0s 295us/step - loss: 0.4469 - acc: 0.9545\n",
      "Epoch 148/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4957 - acc: 0.968 - 0s 317us/step - loss: 0.4443 - acc: 0.9773\n",
      "Epoch 149/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4497 - acc: 0.906 - 0s 317us/step - loss: 0.3896 - acc: 0.9318\n",
      "Epoch 150/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3652 - acc: 0.937 - 0s 317us/step - loss: 0.3829 - acc: 0.9318\n",
      "Epoch 151/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4244 - acc: 0.937 - 0s 272us/step - loss: 0.4103 - acc: 0.9318\n",
      "Epoch 152/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4072 - acc: 0.875 - 0s 249us/step - loss: 0.3870 - acc: 0.9091\n",
      "Epoch 153/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4192 - acc: 0.843 - 0s 340us/step - loss: 0.4442 - acc: 0.8636\n",
      "Epoch 154/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3270 - acc: 1.000 - 0s 317us/step - loss: 0.3721 - acc: 0.9773\n",
      "Epoch 155/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3171 - acc: 0.906 - 0s 295us/step - loss: 0.3466 - acc: 0.9091\n",
      "Epoch 156/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3544 - acc: 0.968 - 0s 385us/step - loss: 0.3628 - acc: 0.9773\n",
      "Epoch 157/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3976 - acc: 0.937 - 0s 317us/step - loss: 0.3760 - acc: 0.9545\n",
      "Epoch 158/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4168 - acc: 0.906 - 0s 363us/step - loss: 0.4027 - acc: 0.9318\n",
      "Epoch 159/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3010 - acc: 0.968 - 0s 363us/step - loss: 0.3625 - acc: 0.9545\n",
      "Epoch 160/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4042 - acc: 0.937 - 0s 317us/step - loss: 0.3649 - acc: 0.9545\n",
      "Epoch 161/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3508 - acc: 0.968 - 0s 317us/step - loss: 0.3194 - acc: 0.9773\n",
      "Epoch 162/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3670 - acc: 0.968 - 0s 295us/step - loss: 0.3195 - acc: 0.9773\n",
      "Epoch 163/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3933 - acc: 0.937 - 0s 272us/step - loss: 0.3355 - acc: 0.9545\n",
      "Epoch 164/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3215 - acc: 0.968 - 0s 272us/step - loss: 0.3018 - acc: 0.9773\n",
      "Epoch 165/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2904 - acc: 0.968 - 0s 295us/step - loss: 0.2804 - acc: 0.9773\n",
      "Epoch 166/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3363 - acc: 0.968 - 0s 295us/step - loss: 0.3255 - acc: 0.9545\n",
      "Epoch 167/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2252 - acc: 1.000 - 0s 317us/step - loss: 0.2841 - acc: 0.9545\n",
      "Epoch 168/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2864 - acc: 0.968 - 0s 317us/step - loss: 0.2830 - acc: 0.9545\n",
      "Epoch 169/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3065 - acc: 0.875 - 0s 295us/step - loss: 0.3184 - acc: 0.9091\n",
      "Epoch 170/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3145 - acc: 0.968 - 0s 295us/step - loss: 0.2797 - acc: 0.9773\n",
      "Epoch 171/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2506 - acc: 0.968 - 0s 317us/step - loss: 0.2884 - acc: 0.9545\n",
      "Epoch 172/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2342 - acc: 1.000 - 0s 295us/step - loss: 0.2821 - acc: 0.9773\n",
      "Epoch 173/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2726 - acc: 1.000 - 0s 295us/step - loss: 0.2690 - acc: 0.9773\n",
      "Epoch 174/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2695 - acc: 0.968 - 0s 295us/step - loss: 0.2588 - acc: 0.9773\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.3637 - acc: 0.906 - 0s 385us/step - loss: 0.3136 - acc: 0.9318\n",
      "Epoch 176/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2861 - acc: 0.968 - 0s 340us/step - loss: 0.2498 - acc: 0.9773\n",
      "Epoch 177/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2716 - acc: 0.968 - 0s 295us/step - loss: 0.2585 - acc: 0.9773\n",
      "Epoch 178/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3004 - acc: 0.968 - 0s 295us/step - loss: 0.2573 - acc: 0.9773\n",
      "Epoch 179/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2653 - acc: 0.937 - 0s 295us/step - loss: 0.2524 - acc: 0.9545\n",
      "Epoch 180/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2122 - acc: 0.968 - 0s 295us/step - loss: 0.2433 - acc: 0.9773\n",
      "Epoch 181/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1943 - acc: 1.000 - 0s 317us/step - loss: 0.2585 - acc: 0.9773\n",
      "Epoch 182/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2458 - acc: 0.968 - 0s 249us/step - loss: 0.2376 - acc: 0.9773\n",
      "Epoch 183/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2342 - acc: 0.968 - 0s 249us/step - loss: 0.2315 - acc: 0.9773\n",
      "Epoch 184/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2489 - acc: 0.937 - 0s 249us/step - loss: 0.2536 - acc: 0.9318\n",
      "Epoch 185/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1999 - acc: 0.968 - 0s 340us/step - loss: 0.2227 - acc: 0.9773\n",
      "Epoch 186/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2496 - acc: 0.906 - 0s 295us/step - loss: 0.2313 - acc: 0.9318\n",
      "Epoch 187/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.968 - 0s 295us/step - loss: 0.2337 - acc: 0.9545\n",
      "Epoch 188/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2396 - acc: 0.968 - 0s 272us/step - loss: 0.2064 - acc: 0.9773\n",
      "Epoch 189/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1968 - acc: 0.968 - 0s 295us/step - loss: 0.2199 - acc: 0.9773\n",
      "Epoch 190/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1971 - acc: 1.000 - 0s 295us/step - loss: 0.2251 - acc: 0.9773\n",
      "Epoch 191/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1708 - acc: 1.000 - 0s 295us/step - loss: 0.2147 - acc: 0.9545\n",
      "Epoch 192/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1393 - acc: 1.000 - 0s 295us/step - loss: 0.2062 - acc: 0.9545\n",
      "Epoch 193/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1754 - acc: 1.000 - 0s 249us/step - loss: 0.1939 - acc: 0.9773\n",
      "Epoch 194/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2224 - acc: 0.968 - 0s 227us/step - loss: 0.2130 - acc: 0.9773\n",
      "Epoch 195/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2022 - acc: 0.968 - 0s 227us/step - loss: 0.1814 - acc: 0.9773\n",
      "Epoch 196/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1193 - acc: 1.000 - 0s 249us/step - loss: 0.1738 - acc: 0.9773\n",
      "Epoch 197/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1399 - acc: 1.000 - 0s 227us/step - loss: 0.1826 - acc: 0.9773\n",
      "Epoch 198/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2488 - acc: 0.937 - 0s 272us/step - loss: 0.2178 - acc: 0.9545\n",
      "Epoch 199/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2077 - acc: 1.000 - 0s 226us/step - loss: 0.2122 - acc: 0.9773\n",
      "Epoch 200/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1662 - acc: 0.968 - 0s 249us/step - loss: 0.2243 - acc: 0.9545\n"
     ]
    }
   ],
   "source": [
    "from rasa_core.policies import FallbackPolicy, KerasPolicy, MemoizationPolicy\n",
    "from rasa_core.agent import Agent\n",
    "\n",
    "# this will catch predictions the model isn't very certain about\n",
    "# there is a threshold for the NLU predictions as well as the action predictions\n",
    "fallback = FallbackPolicy(fallback_action_name=\"utter_unclear\",\n",
    "                          core_threshold=0.2,\n",
    "                          nlu_threshold=0.1)\n",
    "\n",
    "agent = Agent('domain.yml', policies=[MemoizationPolicy(), KerasPolicy(), fallback])\n",
    "\n",
    "# loading our neatly defined training dialogues\n",
    "training_data = agent.load_data('story.md')\n",
    "\n",
    "agent.train(\n",
    "    training_data,\n",
    "    validation_split=0.0,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "agent.persist('models/EA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 668.84it/s, # trackers=1]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Format: \"dot\" not recognized. Use one of:\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6811b4ada2e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'domain.yml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"story.md\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"sg.png\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m##agent.visualize(\"story.md\", \"story_graph.png\", max_history=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sg.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\rasa_core\\agent.py\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(self, resource_name, output_file, max_history, nlu_training_data, should_merge_nodes, fontsize)\u001b[0m\n\u001b[0;32m    354\u001b[0m         visualize_stories(story_steps, self.domain, output_file, max_history,\n\u001b[0;32m    355\u001b[0m                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlu_training_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                           should_merge_nodes, fontsize)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_agent_is_prepared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\rasa_core\\training\\visualization.py\u001b[0m in \u001b[0;36mvisualize_stories\u001b[1;34m(story_steps, domain, output_file, max_history, interpreter, nlu_training_data, should_merge_nodes, fontsize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[0mpersist_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\rasa_core\\training\\visualization.py\u001b[0m in \u001b[0;36mpersist_graph\u001b[1;34m(G, output_file)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnx_agraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_agraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# convert to a graphviz graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     A.layout(\"dot\", args=\"-Goverlap=false -Gsplines=true -Gconcentrate=true \"\n\u001b[0m\u001b[0;32m    222\u001b[0m                          \"-Gfontname=typewriter\")\n\u001b[0;32m    223\u001b[0m     \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pygraphviz\\agraph.py\u001b[0m in \u001b[0;36mlayout\u001b[1;34m(self, prog, args)\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \"\"\"\n\u001b[0;32m   1398\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dot'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_prog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_layout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pygraphviz\\agraph.py\u001b[0m in \u001b[0;36m_run_prog\u001b[1;34m(self, prog, args)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Format: \"dot\" not recognized. Use one of:\r\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from rasa_core.agent import Agent\n",
    "\n",
    "\n",
    "\n",
    "agent = Agent('domain.yml')\n",
    "agent.visualize(\"story.md\",\"sg.png\",max_history=2)\n",
    "##agent.visualize(\"story.md\", \"story_graph.png\", max_history=1)\n",
    "Image(filename=\"sg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\rasa_nlu\\extractors\\entity_synonyms.py:85: UserWarning: Failed to load synonyms file from 'D:\\Jupyter\\Text Generation - RNN\\Abbrevatin_exp_bot\\./models/EA\\default\\current\\entity_synonyms.json'\n",
      "  \"\".format(entity_synonyms_file))\n"
     ]
    }
   ],
   "source": [
    "#Starting the Bot\n",
    "\n",
    "from rasa_core.agent import Agent\n",
    "agent = Agent.load('models/EA', interpreter=model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your bot is ready to talk! Type your messages here or send 'stop'\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "print(\"Your bot is ready to talk! Type your messages here or send 'stop'\")\n",
    "while True:\n",
    "    a = input()\n",
    "    if a == 'stop':\n",
    "        break\n",
    "    responses = agent.handle_message(a)\n",
    "    for response in responses:\n",
    "        print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
